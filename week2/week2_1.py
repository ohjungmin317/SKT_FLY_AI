# -*- coding: utf-8 -*-
"""Week2-1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QGxZHBNlcEJaPHxkqRS-l0SOneLX7qIV

# week2 day4 2023 07 06

**※ RNN**
"""

import matplotlib.pyplot as plt
plt.rcParams["font.family"] = 'Malgun Gothic'
plt.rcParams["axes.grid"] = True
plt.rcParams["figure.figsize"] = (12,6)
plt.rcParams["axes.formatter.useoffset"] = False
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams["axes.formatter.limits"] = -10000, 10000

!pip install -q git+https://github.com/tensorflow/docs

!pip install -U finance-datareader

import FinanceDataReader as fdr
samsung = fdr.DataReader('017670', '1998-09-01', '2022-10-30')
samsung.shape

samsung.head()

samsung.tail()

"""**데이터 시각화**"""

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(12, 6))
sns.lineplot(y=samsung['Close'], x=samsung.index)
plt.show()

"""**특성 추가**"""

import numpy as np
samsung['3MA'] = np.around(samsung['Close'].rolling(window=3).mean(), 0)
samsung['5MA'] = np.around(samsung['Close'].rolling(window=5).mean(), 0)

samsung['Mid'] = (samsung['High'] + samsung['Low'])/2
samsung.head()

samsung.loc[samsung['Volume']==0]

samsung['Volume'] = samsung['Volume'].replace(0, np.NaN)
samsung.isna().sum(axis=0)

samsung = samsung.dropna()
samsung.isna().sum(axis=0)

"""**데이터 정규화**"""

import pandas as pd
from sklearn.preprocessing import MinMaxScaler,RobustScaler
scaler = RobustScaler()
scale_cols = ['Close', '3MA', '5MA', 'Mid','Volume']
df_scaled = scaler.fit_transform(samsung[scale_cols])
df_scaled = pd.DataFrame(df_scaled)
df_scaled.columns = scale_cols
print(df_scaled[:5])

def make_sequene_dataset(feature, label, window_size):
  feature_list = []
  label_list = []
  for i in range(len(feature)-window_size):
    feature_list.append(feature[i:i+window_size])
    label_list.append(label[i+window_size])
  return np.array(feature_list), np.array(label_list)

feature_cols = ['3MA', '5MA', 'Mid','Volume']
label_cols = [ 'Close' ]
npX = pd.DataFrame(df_scaled, columns=feature_cols).values
npY = pd.DataFrame(df_scaled, columns=label_cols).values
print(npX.shape, npY.shape)

window_size = 20
X_data, Y_data = make_sequene_dataset(npX, npY, window_size)
print(X_data.shape, Y_data.shape)

split = int(len(X_data)*0.8)
X_train = X_data[0:split]
y_train = Y_data[0:split]
X_test = X_data[split:]
y_test = Y_data[split:]
print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape)

from tensorflow import keras
from tensorflow.keras import layers
model = keras.Sequential()
model.add(layers.LSTM(64, activation='tanh', input_shape=(20, 4)))
model.add(layers.Dense(16, activation='relu'))
model.add(layers.Dense(1))
model.summary()

from tensorflow.keras.losses import Huber
from keras.callbacks import EarlyStopping

model.compile(loss='mse', optimizer='adam', metrics=['mae'])
early_stop = EarlyStopping(monitor='val_loss', patience=10)

EPOCHS = 100
BATCH_SIZE = 16
history = model.fit(X_train, y_train,
      epochs=EPOCHS,
      batch_size=BATCH_SIZE,
      validation_data=(X_test, y_test))
      # callbacks=[early_stop])

import matplotlib.pyplot as plt
def plot_history(history):
  his_dict = history.history
  loss = his_dict['loss']
  val_loss = his_dict['val_loss']
  epochs = range(1, len(loss) + 1)
  fig = plt.figure(figsize = (12, 5))
  ax1 = fig.add_subplot(1, 2, 1)
  ax1.plot(epochs, loss, 'b-', label = 'train_loss')
  ax1.plot(epochs, val_loss, 'r-', label = 'val_loss')
  ax1.set_title('train and val loss')
  ax1.set_xlabel('epochs')
  ax1.set_ylabel('loss')
  ax1.legend()
  acc = his_dict['mae']
  val_acc = his_dict['val_mae']
  ax2 = fig.add_subplot(1, 2, 2)
  ax2.plot(epochs, acc, 'b-', label = 'train_mae')
  ax2.plot(epochs, val_acc, 'r-', label = 'val_mae')
  ax2.set_title('train and val mae')
  ax2.set_xlabel('epochs')
  ax2.set_ylabel('mae')
  ax2.legend()
  plt.show()

plot_history(history)

loss, mae = model.evaluate(X_test, y_test)

y_pred = model.predict(X_test)
for i in range(5):
  print('Close: ', y_test[i], ' Predict: ', y_pred[i])

plt.figure(figsize=(12, 6))
plt.ylabel('Close')
plt.xlabel('period')
plt.plot(y_test[20:], label='actual')
plt.plot(y_pred, label='prediction')
plt.grid()
plt.legend(loc='best')
plt.show()

"""**※ LSTM**"""

from tensorflow import keras
from tensorflow.keras import layers
model = keras.Sequential()
model.add(layers.LSTM(64, activation='tanh',
 return_sequences=True,
 input_shape=(20, 4)))
model.add(layers.LSTM(128, activation='tanh',
 return_sequences=False))
model.add(layers.Dense(16, activation='relu'))
model.add(layers.Dense(1))
model.summary()

from tensorflow.keras.losses import Huber
from keras.callbacks import EarlyStopping
model.compile(loss=Huber(), optimizer='adam', metrics=['mae'])
early_stop = EarlyStopping(monitor='val_loss', patience=10)
EPOCHS = 100
BATCH_SIZE = 16
history = model.fit(X_train, y_train,
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    validation_data=(X_test, y_test),
    callbacks=[early_stop])

y_pred = model.predict(X_test)
for i in range(5):
  print('Close: ', y_test[i], ' Predict: ', y_pred[i])

plt.figure(figsize=(12, 6))
plt.ylabel('Close')
plt.xlabel('period')
plt.plot(y_test[20:], label='actual')
plt.plot(y_pred, label='prediction')
plt.grid()
plt.legend(loc='best')
plt.show()

mape2 = np.sum(abs(y_test-y_pred)/y_test) / len(y_test)

"""**※ RNN으로 이미지 분류**"""

from tensorflow.keras.datasets import mnist
(X_train, y_train), (X_test, y_test) = mnist.load_data()
print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape)

import matplotlib.pyplot as plt
_, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 7))
for ax, image, label in zip(axes, X_train, y_train):
  ax.set_axis_off()
  ax.imshow(image, cmap='gray')
  ax.set_title(label)

import numpy as np
X_train = X_train / 255.
X_test = X_test / 255.
print(np.min(X_train), np.max(X_train))

from tensorflow.keras.utils import to_categorical
y_train_oh = to_categorical(y_train)
y_test_oh = to_categorical(y_test)
y_train_oh[:5]

from tensorflow import keras
from tensorflow.keras import layers
model = keras.Sequential([
layers.LSTM(64, input_shape=(28, 28)),
layers.Dense(10, activation='softmax')
])
model.summary()

model.compile(loss='categorical_crossentropy',
 optimizer='adam',
 metrics=['accuracy'])
epochs = 30
batch_size = 64
history = model.fit(X_train, y_train_oh,
 validation_data=(X_test, y_test_oh),
    epochs=epochs,
    batch_size=batch_size,
    verbose=1)

plot_history(history)

loss, acc = model.evaluate(X_test, y_test_oh, verbose=1)

y_pred = model.predict(X_test)
y_pred = np.argmax(y_pred, axis=1)
print(y_pred[:10])

"""**※ fashion mnist Lstm**"""

from tensorflow.keras.datasets import fashion_mnist
(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()
print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape)

import matplotlib.pyplot as plt
_, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 7))
for ax, image, label in zip(axes, X_train, y_train):
  ax.set_axis_off()
  ax.imshow(image, cmap='gray')
  ax.set_title(label)

import numpy as np
X_train = X_train / 255.
X_test = X_test / 255.
print(np.min(X_train), np.max(X_train))

from tensorflow.keras.utils import to_categorical
y_train_oh = to_categorical(y_train)
y_test_oh = to_categorical(y_test)
y_train_oh[:5]

from tensorflow import keras
from tensorflow.keras import layers
model = keras.Sequential([
layers.LSTM(64, input_shape=(28, 28)),
layers.Dense(10, activation='softmax')
])
model.summary()

model.compile(loss='categorical_crossentropy',
 optimizer='adam',
 metrics=['accuracy'])
epochs = 30
batch_size = 64
history = model.fit(X_train, y_train_oh,
 validation_data=(X_test, y_test_oh),
    epochs=epochs,
    batch_size=batch_size,
    verbose=1)

plot_history(history)

loss, acc = model.evaluate(X_test, y_test_oh, verbose=1)

y_pred = model.predict(X_test)
y_pred = np.argmax(y_pred, axis=1)
print(y_pred[:10])

"""**※ IMDB 긍정/부정 분류**"""

from keras.datasets import imdb
import numpy as np
import keras

(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=500)

print(X_train.shape, X_test.shape)
print(X_test.shape, y_test.shape)

print(X_train[0])
print('length :', len(X_train[0]))

print(y_train[:10])

print(imdb.get_word_index())

word_index = imdb.get_word_index()
reverse_word_index = dict(
    [(value,key) for (key, value) in word_index.items()])
print(reverse_word_index)

decode_review = ' '.join(
    [reverse_word_index.get(i-3, '?') for i in X_train[0]]
)
print(decode_review)

lengths = np.array([len(x) for x in X_train])
print(lengths[:20])

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 7))
plt.hist(lengths)
plt.xlabel('length')
plt.ylabel('frequency')
plt.show()

from tensorflow.keras.preprocessing.sequence import pad_sequences

max_len = 200
X_train = pad_sequences(X_train, maxlen=max_len)

print(X_train.shape)

print(X_train[:2])

from tensorflow.keras.utils import to_categorical

train_oh = to_categorical(X_train)

print(train_oh.shape)

"""### 7. 모델만들기(LSTM 사용)"""

from tensorflow import keras

model = keras.Sequential()

model.add(keras.layers.Embedding(500, 16, input_length=200))
model.add(keras.layers.LSTM(64, return_sequences=True))
model.add(keras.layers.LSTM(32))
model.add(keras.layers.Dense(1, activation='sigmoid'))

model.summary()

model.compile(optimizer='adam',
        loss='binary_crossentropy',
        metrics=['acc'])

checkpoint_path = './imdb_checkpoint.ckpt'
checkpoint = keras.callbacks.ModelCheckpoint(checkpoint_path,
                save_weights_only=True, save_best_only=True,
                monitor='val_loss', verbose=1)
early_stop = keras.callbacks.EarlyStopping(
                                 patience=10, monitor='val_loss')
EPOCHS = 100
BATCH_SIZE = 64
history = model.fit(X_train, y_train,
                epochs=EPOCHS,
                batch_size=BATCH_SIZE,
                validation_split=0.2,
                callbacks=[checkpoint, early_stop])

model.load_weights(checkpoint_path)

X_test = pad_sequences(X_test, maxlen=max_len)
print(X_test.shape)

y_pred = model.predict(X_test)
print(y_pred[:10])

y_pred = (y_pred >0.5).astype('int').reshape(-1)
print(y_pred[:10])

from sklearn.metrics import accuracy_score, precision_score, recall_score
from sklearn.metrics import f1_score

def print_scores(y_true, y_pred):
   print('accuracy_score: {0:.4f}'.format(accuracy_score(y_true, y_pred)))
   print('precision_score: {0:.4f}'.format(precision_score(y_true, y_pred)))
   print('recall_score: {0:.4f}'.format(recall_score(y_true, y_pred)))

temp_str = " I know this is going to anger some people,but I hated toby McGuire as Spiderman.He was a whiny, scrawny little baby.I don't hate those movies but he's just not a goodSpiderman. Tom Holland on the other hand, is probably the onlySpiderman actor that I will ever rate is completely and utterlyperfect. I like the addition of the smart suit and other thingsthat are linked Iron Man, because the connection between Peter andStark is full of heart. Keep 'em coming."

import re

new_sentence = re.sub('[^0-9a-zA-Z ]', '', temp_str).lower()
new_sentence

encoded = []

for word in new_sentence.split():
   try:
    if word_index[word] <= 500:
     encoded.append(word_index[word]+3)
    else:
     encoded.append(2)
   except KeyError:
    encoded.append(2)

print(encoded)

"""###14-1. 시퀀스 데이터
- 파일로 저장되어 있는 최상의 상태를 모델로 복원
"""

max_len = 200
pad_new = pad_sequences([encoded], maxlen = max_len)
pad_new

"""###15. 예측
- 복원된 모델을 사용해서 테스트 데이터를 예측하고 평가지표 계산
"""

y_pred = model.predict(X_test)
y_pred_argmax = np.argmax(y_pred, axis=1)
#print_score(y_test, y_pred_argmax)

score = float(model.predict(pad_new)) # 예측
if(score >0.5):
  print("{:.3f}% 확률로 긍정 리뷰".format(score * 100))
else:
  print("{:.3f}% 확률로 부정 리뷰".format((1 - score) * 100))

"""## 실습 (lstm 사용하여 뉴스)"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Embedding
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing import sequence
from tensorflow.keras.datasets import reuters
from tensorflow.keras.callbacks import EarlyStopping
import numpy as np
import matplotlib.pyplot as plt

(X_train, y_train), (X_test, y_test) = reuters.load_data(num_words=1000,
                                                         test_split=0.2)
print(X_train.shape, X_test.shape)
print(X_test.shape, y_test.shape)

print(X_train[0])
print('length :', len(X_train[0]))

len(set(y_train))

print(reuters.get_word_index())

word_index = reuters.get_word_index()
reverse_word_index = dict(
    [(value,key) for (key, value) in word_index.items()])
print(reverse_word_index)

decode_review = ' '.join(
    [reverse_word_index.get(i-3, '?') for i in X_train[0]]
)
print(decode_review)

lengths = np.array([len(x) for x in X_train])
print(lengths[:20])

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 7))
plt.hist(lengths)
plt.xlabel('length')
plt.ylabel('frequency')
plt.show()

from tensorflow.keras.preprocessing.sequence import pad_sequences
max_len = 200
X_train = pad_sequences(X_train, maxlen=max_len)

print(X_train.shape)

print(X_train[:2])

from tensorflow.keras.utils import to_categorical

train_oh = to_categorical(X_train)
print(train_oh.shape)

from tensorflow import keras

model = keras.Sequential()

model.add(keras.layers.Embedding(1000, 32, input_length=200))
model.add(keras.layers.LSTM(64, return_sequences=True))
model.add(keras.layers.LSTM(64))
model.add(keras.layers.Dense(46, activation='softmax'))

model.summary()

#모델 실행 옵션
model.compile(optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=['acc'])

checkpoint_path = './imdb_checkpoint.ckpt'
checkpoint = keras.callbacks.ModelCheckpoint(checkpoint_path,
                save_weights_only=True, save_best_only=True,
                monitor='val_loss', verbose=1)
early_stop = keras.callbacks.EarlyStopping(
                                 patience=5, monitor='val_loss')

#모델 실행
EPOCHS = 100
BATCH_SIZE = 50
history = model.fit(X_train, y_train,
                epochs=EPOCHS,
                batch_size=BATCH_SIZE,
                validation_split=0.2,
                callbacks=[checkpoint, early_stop])

model.load_weights(checkpoint_path)
X_test = pad_sequences(X_test, maxlen=max_len)
print("\n Test Accuracy : %.4f" % (model.evaluate(X_test, y_test.astype(np.float32))[1]))

#학습셋과 테스트셋 오차
y_vloss = history.history['val_loss']
y_loss = history.history['loss']

x_len = np.arange(len(y_loss))
plt.plot(x_len, y_vloss, marker='.', c='red', label='Testset loss')
plt.plot(x_len, y_loss, marker='.', c='blue', label='Trainset loss')
plt.legend(loc='upper right')
plt.grid()
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.show()

"""#week2 day5 2023 07 06

### ※ keras 모델 구성

####함수형 API
"""

from keras.models import Sequential, Model
from keras import layers
from keras import Input

input_tensor = Input(shape=(64,))
x = layers.Dense(32, activation='relu')(input_tensor)
x = layers.Dense(32, activation='relu')(x)
output_tensor = layers.Dense(10, activation='softmax')(x)
model = Model(input_tensor, output_tensor)
model.summary()

model.compile(optimizer='rmsprop', loss='categorical_crossentropy')
import numpy as np
x_train= np.random.random((1000, 64))
y_train= np.random.random((1000, 10))

model.fit(x_train, y_train, epochs=10, batch_size=128)
source = model.evaluate(x_train, y_train)

print(source)

"""#### ※ Squential()의 객체를 만들어서 레이어를 추가하는 방법으로 레이어를 구성"""

from tensorflow import keras
from tensorflow.keras import layers
model = keras.Sequential()
model.add(layers.Conv2D(64, (3, 3), activation='relu',
input_shape=(28, 28, 1)))
model.add(layers.Conv2D(32, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1)))
model.add(layers.Flatten())
model.add(layers.Dense(32, activation = 'relu'))
model.add(layers.Dense(10, activation = 'softmax'))
model.summary()

from tensorflow.keras.utils import plot_model
plot_model(model)

model = keras.Sequential([
layers.Conv2D(64, (3, 3), activation='relu',
                                       input_shape=(28, 28, 1)),
layers.Conv2D(32, (3, 3), activation='relu'),
layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1)),
layers.Flatten(),
layers.Dense(32, activation = 'relu'),
layers.Dense(10, activation = 'softmax')])
model.summary()

"""#### ※ Sequentia() [ 파라미터 개수보다도 내 모델에 적합한지에 대해 확인을 해줘야 한다. ]"""

from tensorflow.keras.models import Model
from tensorflow.keras import layers

inputs = layers.Input(shape=(28, 28, 1))
x = layers.Conv2D(64, (3, 3), activation='relu')(inputs)
x = layers.Conv2D(32, (3, 3), activation='relu')(x)
x = layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1))(x)
x = layers.Flatten()(x)
x = layers.Dense(32, activation = 'relu')(x)

outputs = layers.Dense(10, activation = 'softmax')(x)
model = Model(inputs=inputs, outputs=outputs)
model.summary()

plot_model(model, show_shapes=True)

from tensorflow.keras.models import Model
from tensorflow.keras import layers

# First Inputs
inputX = layers.Input(shape=(32,), name="x_input")
x = layers.Dense(8, activation="relu")(inputX)
x = layers.Dense(4, activation="relu")(x)
x = Model(inputs=inputX, outputs=x)

# Second Inputs
inputY = layers.Input(shape=(128,), name="y_input")
y = layers.Dense(64, activation="relu")(inputY)
y = layers.Dense(32, activation="relu")(y)
y = layers.Dense(4, activation="relu")(y)
y = Model(inputs=inputY, outputs=y)

# Combine
combined = layers.concatenate([x.output, y.output])
z = layers.Dense(2, activation="relu")(combined)
z_output = layers.Dense(1, activation="linear",
                           name="z_output")(z)

plot_model(model, show_shapes=True)

# First Inputs
inputX = layers.Input(shape=(32,), name="x_input")
x = layers.Dense(8, activation="relu")(inputX)
x = Model(inputs=inputX, outputs=x)

# Second Inputs
inputY = layers.Input(shape=(128,), name="y_input")
y = layers.Dense(64, activation="relu")(inputY)
y = Model(inputs=inputY, outputs=y)

# Combine
combined = layers.concatenate([x.output, y.output])

# First Output
y_output = layers.Dense(4, activation="relu",
name="y_output")(combined)

# Second Output
z_output = layers.Dense(1, activation="linear",
name="z_output")(combined)

model = Model(inputs=[x.input, y.input],
              outputs=[y_output, z_output])

plot_model(model, show_shapes=True)

"""#### ※ 다중 입력 모델/ 다중 출력 모델 [ 과적합이 되기 때문에 dropout을 사용 ]"""

from keras.models import Model
from keras import layers
from keras import Input

text_vocabulary_size = 10000
question_vocabulary_size = 10000
answer_vocabulary_size = 500

#텍스트 입력은 길이가 정해지지 않은 정수 시퀀스입니다. 입력 이름을 지정하 수 있습니다.
text_input = Input(shape=(None,), dtype='int32', name='text')
#입력을 크기가 64인 벡터의 시퀀스로 임베딩합니다.
embdded_text = layers.Embedding(text_vocabulary_size, 64)(text_input)
#LSTM으 사용하여 이 벡터들을 하나의 벡터로 인툉합니다.
encoded_text = layers.LSTM(32)(embdded_text)

#질문도 동일한 과정을 거칩니다.
question_input = Input(shape=(None,), dtype='int32', name = 'question')
#입력을 크기가 32인 벡터의 시퀀스로 임베딩합니다.
embdded_question = layers.Embedding(text_vocabulary_size, 32)(question_input)
#LSTM으 사용하여 이 벡터들을 하나의 벡터로 인툉합니다.
encoded_question = layers.LSTM(16)(embdded_question)

#인코딩된 질문과 테그트를 연결합니다.
concatenated = layers.concatenate([encoded_text, encoded_question])


#소프트맥스 분류기를 추가합니다.
answer = layers.Dense(answer_vocabulary_size, activation='softmax')(concatenated)

model = Model([text_input, question_input], answer)
model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])

model.summary()

from tensorflow.keras.utils import plot_model

plot_model(model)

from keras.utils import to_categorical

num_samples = 1000
max_length = 100

# 입력으로 사용할 Data를 Random하게 Shpae를 맞춰서 선언한다.
text = np.random.randint(1, text_vocabulary_size, size=(num_samples, max_length))
question = np.random.randint(1, question_vocabulary_size, size=(num_samples, max_length))

# Target Data를 원핫 인코딩을 통하여 구성한다.
answers = np.random.randint(0, answer_vocabulary_size, size=num_samples)
answers = to_categorical(answers)
model.summary()

from tensorflow.keras.utils import plot_model

plot_model(model)

from keras.models import Sequential, Model
from keras import layers
from keras import Input

# Sequential model
seq_model = Sequential()
seq_model.add(layers.Dense(32, activation='relu', input_shape=(64,)))
seq_model.add(layers.Dense(32, activation='relu'))
seq_model.add(layers.Dense(10, activation='softmax'))

# Functianal API
input_tensor = Input(shape=(64,))
x = layers.Dense(32, activation='relu')(input_tensor)
x = layers.Dense(32, activation='relu')(x)
output_tensor = layers.Dense(10, activation='softmax')(x)

model = Model(input_tensor, output_tensor)

# 컴파일
model.compile(optimizer='rmsprop', loss='categorical_crossentropy')

import numpy as np
x_train = np.random.random((1000, 64))
y_train = np.random.random((1000, 10))

# 훈련
model.fit(x_train, y_train, epochs=10, batch_size = 128)

# 평가
score = model.evaluate(x_train, y_train)

"""### mnist를 이용하여 다중입력"""

from keras.models import Sequential, Model
from keras import layers
from keras import Input
from tensorflow.keras.datasets import mnist
import tensorflow as tf

mnist = tf.keras.datasets.mnist
(X_train, y_train), (X_test, y_test) = mnist.load_data()

print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape)

X_train[0]

type(X_train[0])

import matplotlib.pyplot as plt

plt.figure(figsize=(1,1))
plt.imshow(X_train[0], cmap='gray')

y_train[0]

from tensorflow.keras.models import Model
from tensorflow.keras import layers

inputs = layers.Input(shape=(28,28))

x = layers.Flatten()(inputs)
x = layers.Dense(256, activation='relu')(x)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dropout(0.3)(x)
x = layers.Dense(64, activation='relu')(x)
x = layers.Dense(32, activation='relu')(x)
x = layers.Dropout(0.3)(x)

outputs = layers.Dense(10, activation = 'softmax')(x)


model = Model(inputs=inputs, outputs=outputs)
model.summary()

from tensorflow.keras.utils import plot_model

plot_model(model, show_shapes=True)

model.compile(
    optimizer = 'adam',
    loss = 'sparse_categorical_crossentropy',
    metrics = ['acc']
)
from keras.callbacks import EarlyStopping

early_stop = EarlyStopping(monitor='val_loss', patience=10)

EPOCHS = 100
BATCH_SIZE = 64

history = model.fit(X_train, y_train,
                    epochs = EPOCHS,
                    batch_size = BATCH_SIZE,
                    validation_split = 0.2,
                    callbacks = [early_stop])

def plot_history(history):
  his_dict = history.history
  lose

history.history.keys()

loss = history.history['loss']
val_loss = history.history['val_loss']

acc = history.history['acc']
val_acc = history.history['val_acc']

epochs = range(1, len(loss)+1)

fig = plt.figure(figsize=(12,5))

ax1 = fig.add_subplot(1,2,1)
ax1.plot(epochs, loss, 'b-', label='train_loss')
ax1.plot(epochs, val_loss, 'r-', label='val_loss')
ax1.set_xlabel('epochs')
ax1.set_ylabel('loss')
ax1.set_title('train and val loss')
ax1.legend()

ax2 = fig.add_subplot(1,2,2)
ax2.plot(epochs, acc, 'b-', label='train_acc')
ax2.plot(epochs, val_acc, 'r-', label='val_acc')
ax2.set_xlabel('epochs')
ax2.set_ylabel('accuracy')
ax2.set_title('train and val accuracy')
ax2.legend()

plt.show()

y_pred = model.predict(X_test)

print(y_pred.shape)

import numpy as np
y_pred = np.argmax(y_pred, axis=1)
print(y_pred.shape)

#평가지표 계산
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

def print_score(y_test, y_pred):
  print("accuracy : %.3f" %(accuracy_score(y_test, y_pred)))
  print("precision : %.3f" %(precision_score(y_test, y_pred, average='macro')))
  print("recall_score : %.3f" %(recall_score(y_test, y_pred, average='macro')))
  print("f1_score : %.3f" %(f1_score(y_test, y_pred, average='macro')))

print_score(y_test, y_pred)

"""### iris를 이용하여 다중입력"""

from keras.models import Sequential, Model
from keras import layers
from keras import Input
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.neighbors import KNeighborsClassifier
import pandas as pd
import seaborn as sns

iris = sns.load_dataset('iris')

X = iris.drop('species', axis=1)
y = iris['species']

label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42)

print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape)

X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.2, random_state=42)

print(X_train.shape, y_train.shape)
print(X_val.shape, y_val.shape)
print(X_test.shape, y_test.shape)

from tensorflow.keras.models import Model
from tensorflow.keras import layers

inputs = layers.Input(shape=(4,))

x = layers.Dense(64,activation='relu')(inputs)
x = layers.Dense(64,activation='relu')(x)
x = layers.Dropout(0.2)(x)

outputs = layers.Dense(3,activation='softmax')(x)

model = Model(inputs=inputs,outputs=outputs)
model.summary()

from tensorflow.keras.utils import plot_model

plot_model(model, show_shapes=True)

model.compile(
    optimizer = 'adam',
    loss = 'sparse_categorical_crossentropy',
    metrics = ['acc']
)

from keras.callbacks import EarlyStopping

early_stop = EarlyStopping(monitor='val_loss', patience=10)

EPOCHS = 100
BATCH_SIZE = 64

history = model.fit(X_train, y_train,
                    epochs = EPOCHS,
                    batch_size = BATCH_SIZE,
                    validation_data = (X_val,y_val),
                    callbacks = [early_stop])

def plot_history(history):
  his_dict = history.history
  lose

history.history.keys()

loss = history.history['loss']
val_loss = history.history['val_loss']

acc = history.history['acc']
val_acc = history.history['val_acc']

epochs = range(1, len(loss)+1)

fig = plt.figure(figsize=(12,5))

ax1 = fig.add_subplot(1,2,1)
ax1.plot(epochs, loss, 'b-', label='train_loss')
ax1.plot(epochs, val_loss, 'r-', label='val_loss')
ax1.set_xlabel('epochs')
ax1.set_ylabel('loss')
ax1.set_title('train and val loss')
ax1.legend()

ax2 = fig.add_subplot(1,2,2)
ax2.plot(epochs, acc, 'b-', label='train_acc')
ax2.plot(epochs, val_acc, 'r-', label='val_acc')
ax2.set_xlabel('epochs')
ax2.set_ylabel('accuracy')
ax2.set_title('train and val accuracy')
ax2.legend()

plt.show()

y_pred = model.predict(X_test)

print(y_pred.shape)

import numpy as np
y_pred = np.argmax(y_pred, axis=1)
print(y_pred.shape)

#평가지표 계산
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

def print_score(y_test, y_pred):
  print("accuracy : %.3f" %(accuracy_score(y_test, y_pred)))
  print("precision : %.3f" %(precision_score(y_test, y_pred, average='macro')))
  print("recall_score : %.3f" %(recall_score(y_test, y_pred, average='macro')))
  print("f1_score : %.3f" %(f1_score(y_test, y_pred, average='macro')))

print_score(y_test, y_pred)

"""#### ※ 노이즈 제거"""

from keras.models import Sequential, Model
from keras import layers
from keras import Input
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.neighbors import KNeighborsClassifier
import pandas as pd
import seaborn as sns

import tensorflow as tf

# 1. Fashion MNIST 데이터셋 불러오기
fashion_mnist = tf.keras.datasets.fashion_mnist
(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()

(X_train, _), (X_test, _) = fashion_mnist.load_data()
X_train = X_train.astype('float32') / 255.
X_test = X_test.astype('float32') / 255.
X_train = X_train[..., tf.newaxis]
X_test = X_test[..., tf.newaxis]
print(X_train.shape)
print(X_test.shape)

noise_factor = 0.2

X_train_noisy = X_train + noise_factor * tf.random.normal(shape=X_train.shape)
X_test_noisy = X_test + noise_factor * tf.random.normal(shape=X_test.shape)
X_train_noisy = tf.clip_by_value(X_train_noisy,clip_value_min=0., clip_value_max=1.)
X_test_noisy = tf.clip_by_value(X_test_noisy,clip_value_min=0., clip_value_max=1.)

import matplotlib.pyplot as plt

n=5
plt.figure(figsize=(10, 2))
for i in range(n):
  ax = plt.subplot(1, n, i + 1)
  plt.imshow(tf.squeeze(X_test_noisy[i]))
  plt.gray()
plt.show()

from tensorflow.keras.models import Model
class Denoise(Model):
  def __init__(self):
   super(Denoise, self).__init__()
   self.encoder = tf.keras.Sequential([
    layers.Input(shape=(28, 28, 1)),
    layers.Conv2D(16, (3, 3), activation='relu',
                  padding='same', strides=2),
    layers.Conv2D(8, (3, 3), activation='relu',
                  padding='same', strides=2)])
   self.decoder = tf.keras.Sequential([
     layers.Conv2DTranspose(8, kernel_size=3, strides=2,
                           activation='relu', padding='same'),
     layers.Conv2DTranspose(16, kernel_size=3, strides=2,
                           activation='relu', padding='same'),
     layers.Conv2D(1, kernel_size=(3, 3), activation='sigmoid',
                           padding='same')])
  def call(self, x):
     encoded = self.encoder(x)
     decoded = self.decoder(encoded)
     return decoded

from tensorflow.keras import losses

model = Denoise()
model.compile(optimizer='adam', loss=losses.MeanSquaredError())
model.fit(X_train_noisy, X_train,
         epochs=10,
         shuffle=True,
         validation_data=(X_test_noisy, X_test))

encoded_imgs = model.encoder(X_test_noisy).numpy()
decoded_imgs = model.decoder(encoded_imgs).numpy()

n= 5
plt.figure(figsize=(10, 4))
for i in range(n):
   # display original + noise
   ax = plt.subplot(2, n, i + 1)
   plt.title("original + noise")
   plt.imshow(tf.squeeze(X_test_noisy[i]))
   ax.get_xaxis().set_visible(False)
   ax.get_yaxis().set_visible(False)

   # display reconstruction
   bx = plt.subplot(2, n, i + n + 1)
   plt.title("reconstructed")
   plt.imshow(tf.squeeze(decoded_imgs[i]))
   bx.get_xaxis().set_visible(False)
   bx.get_yaxis().set_visible(False)
   plt.show()

"""#### ※ 노이즈 제거 (mnist)

"""

import tensorflow as tf

# 1. Fashion MNIST 데이터셋 불러오기
mnist = tf.keras.datasets.mnist
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

(X_train, _), (X_test, _) = mnist.load_data()
X_train = X_train.astype('float32') / 255.
X_test = X_test.astype('float32') / 255.
X_train = X_train[..., tf.newaxis]
X_test = X_test[..., tf.newaxis]
print(X_train.shape)
print(X_test.shape)

noise_factor = 0.2

X_train_noisy = X_train + noise_factor * tf.random.normal(shape=X_train.shape)
X_test_noisy = X_test + noise_factor * tf.random.normal(shape=X_test.shape)
X_train_noisy = tf.clip_by_value(X_train_noisy,clip_value_min=0., clip_value_max=1.)
X_test_noisy = tf.clip_by_value(X_test_noisy,clip_value_min=0., clip_value_max=1.)

import matplotlib.pyplot as plt

n=3
plt.figure(figsize=(10, 2))
for i in range(n):
  ax = plt.subplot(1, n, i + 1)
  plt.imshow(tf.squeeze(X_test_noisy[i]))
  plt.gray()
plt.show()

class Denoise(Model):
  def __init__(self):
   super(Denoise, self).__init__()
   self.encoder = tf.keras.Sequential([
    layers.Input(shape=(28, 28, 1)),
    layers.Conv2D(16, (3, 3), activation='relu',
                  padding='same', strides=1),
    layers.Conv2D(8, (3, 3), activation='relu',
                  padding='same', strides=1)])
   self.decoder = tf.keras.Sequential([
     layers.Conv2DTranspose(8, kernel_size=3, strides=1,
                           activation='relu', padding='same'),
     layers.Conv2DTranspose(16, kernel_size=3, strides=1,
                           activation='relu', padding='same'),
     layers.Conv2D(1, kernel_size=(3, 3), activation='sigmoid',
                           padding='same')])
  def call(self, x):
     encoded = self.encoder(x)
     decoded = self.decoder(encoded)
     return decoded

from keras.callbacks import EarlyStopping

early_stop = EarlyStopping(monitor='val_loss', patience=10)

from tensorflow.keras import losses

model = Denoise()
model.compile(optimizer='adam', loss=losses.MeanSquaredError())
model.fit(X_train_noisy, X_train,
         epochs=100,
         shuffle=True,
         validation_data=(X_test_noisy, X_test),
          callbacks=[early_stop])

encoded_imgs = model.encoder(X_test_noisy).numpy()
decoded_imgs = model.decoder(encoded_imgs).numpy()

n= 5
plt.figure(figsize=(10, 4))
for i in range(n):
   # display original + noise
   ax = plt.subplot(2, n, i + 1)
   plt.title("original + noise")
   plt.imshow(tf.squeeze(X_test_noisy[i]))
   ax.get_xaxis().set_visible(False)
   ax.get_yaxis().set_visible(False)

   # display reconstruction
   bx = plt.subplot(2, n, i + n + 1)
   plt.title("reconstructed")
   plt.imshow(tf.squeeze(decoded_imgs[i]))
   bx.get_xaxis().set_visible(False)
   bx.get_yaxis().set_visible(False)
   plt.show()

from keras.models import Sequential, Model
from keras import layers
from keras import Input
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.neighbors import KNeighborsClassifier
import pandas as pd
import seaborn as sns

