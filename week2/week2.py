# -*- coding: utf-8 -*-
"""Week2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1V88gffSWe9NR4ZWGVvgIHK88oHk-o9UO

# week2 day1 2023 07 03
"""

# 1.간단한 데이터셋 생성
X = [[0., 0.], [1., 1.]]
y = [[0, 1],[1, 1]]

from sklearn.neural_network import MLPClassifier

clf = MLPClassifier(solver = "lbfgs", alpha = 1e-5, hidden_layer_sizes = (5,2), random_state = 1)
clf.fit(X,y)

clf.predict([[2.,2.],[-1.,-2]])

clf.coefs_

[coef.shape for coef in  clf.coefs_]

import matplotlib.pyplot as plt
import numpy as np
x = 2 * np.random.rand(100,1) # [0, 1) 범위에서 균일한 분포 100 X 1 array
y = 4 + 3*x + np.random.randn(100,1) # normal distribution(mu=0,var=1)분포 100 X 1 array
plt.scatter(x,y)
plt.show()

x_b = np.c_[np.ones((100,1)),x] # 모든 샘플에 index 0번에 1을 추가

# np.linalg.inv는 넘파이 선형대수 모듈(linalg)의 inv(역함수)
# .dot은 행렬 곱셈
theta_best = np.linalg.inv(x_b.T.dot(x_b)).dot(x_b.T).dot(y)
theta_best

x_new = np.array([[0],[2]])
x_new_b = np.c_[np.ones((2,1)),x_new]
prediction = x_new_b.dot(theta_best)
prediction

plt.plot(x_new,prediction,"r-")
plt.plot(x,y,"b.")
plt.axis([0,2,0,15]) # x축 범위 0~2, y축 범위 0~15
plt.show()

from sklearn.linear_model import LinearRegression
lin_reg = LinearRegression()
lin_reg.fit(x,y)
print(lin_reg.intercept_,lin_reg.coef_)

print(lin_reg.predict(x_new))

import numpy as np

x = 2 * np.random.rand(100,1) # 100 x 1 크기의 0~1의 균일분포
x_b = np.c_[np.ones((100,1)),x] # bias(1)를 전체 데이터에 추가
y = 4 + 3*np.random.randn(100,1) # 100 x 1 크기의 표준정규분포 추출

learning_rate = 0.001
iterations = 1000
m = x_b.shape[0] # 100개 (x 데이터)

theta = np.random.randn(2,1) # 2x1 크기의 평균 0, 분산1 정규 분포 추출

for iteration in range(iterations):
  gradients = 2/m * x_b.T.dot(x_b.dot(theta)-y)
  theta = theta - (learning_rate * gradients)

theta

import numpy as np
data_num = 1000
x = 3 * np.random.rand(data_num,1) - 1
y = 0.2 * (x**2) + np.random.randn(1000,1)

from sklearn.preprocessing import PolynomialFeatures
poly_features = PolynomialFeatures(degree=2,include_bias=False)
x_poly = poly_features.fit_transform(x)
print(x[0])
print(x_poly[0])

from sklearn.linear_model import LinearRegression
lin_reg = LinearRegression()
lin_reg.fit(x_poly,y)
print(lin_reg.intercept_,lin_reg.coef_)

import pandas as pd
import matplotlib.pyplot as plt
import matplotlib
matplotlib.rcParams['axes.unicode_minus'] = False ## 마이나스 '-' 표시 제대로 출력

from statsmodels.formula.api import ols
from sklearn.linear_model import LinearRegression

#df = pd.read_csv('./toluca_company_dataset.csv') ## 데이터 불러오기

df.head(5)

fig = plt.figure(figsize=(8,8))
fig.set_facecolor('white')

font_size = 15
plt.scatter(df['Lot_size'],df['Work_hours']) ## 원 데이터 산포도

plt.xlabel('Lot Size', fontsize=font_size)
plt.ylabel('Work Hours',fontsize=font_size)
plt.show()

fit = ols('Work_hours ~ Lot_size',data=df).fit() ## 단순선형회귀모형 적합

fit.summary()

fit= ols('Work_hours ~ Lot_size -1',data=df).fit()

fit.fittedvalues

# 모델검증에 필요한 잔차

fit.resid

#예측값
fit.predict(exog = dict(Lot_size =[80]))

## 시각화
fig = plt.figure(figsize=(8,8))
fig.set_facecolor('white')

font_size = 15
plt.scatter(df['Lot_size'],df['Work_hours']) ## 원 데이터 산포도
plt.plot(df['Lot_size'],fit.fittedvalues,color='red') ## 회귀직선 추가

plt.xlabel('Lot Size', fontsize=font_size)
plt.ylabel('Work Hours',fontsize=font_size)
plt.show()

## 잔차도 Residual Plot
fig = plt.figure(figsize=(8,8))
fig.set_facecolor('white')

font_size = 15

plt.scatter(df['Lot_size'],fit.resid) ## 잔차도 출력

plt.xlabel('Lot Size', fontsize=font_size)
plt.ylabel('Residual', fontsize=font_size)
plt.show()

## sklearn linear regression 사용
x = df['Lot_size'].values.reshape(-1,1) ## 차원 증가 시켜준다.
y = df['Work_hours']

fit = LinearRegression().fit(x,y) ## 단순선형회귀모형 적합

# 회귀 계수
print('절편:',fit.intercept_)
print('기울기:', fit.coef_)

fit.predict(x)

residual = y - fit.predict(x)
print(residual)

fit.predict([[80]])

import numpy as np

#무작위로 선형 데이터셋 생성
X = 2 * np.random.rand(100,1)  #rand: 난수 생성 함수
y = 4 +  3 * X + np.random.randn(100,1) #randn은 정규분포에 대한 난수 생성

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split


lin_reg = LinearRegression()
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

lin_reg.fit(X_train, y_train)

print(lin_reg.intercept_)  #편향(절편)
print(lin_reg.coef_)       #가중치
print(lin_reg.score(X_train, y_train)) #train set 점수
print(lin_reg.score(X_test, y_test)) #test set 점수

import pandas as pd

boston = pd.read_csv('./Boston_house.csv')
boston.head()

# Commented out IPython magic to ensure Python compatibility.
# 범죄율, 주택당 방 수, 인구 중 하위 계층 비율, 노후 주택 비율과 주택가격과의 상관관계를 예측

from sklearn.model_selection import train_test_split

X = boston[['CRIM','RM', 'LSTAT', 'AGE']]
y = boston[['Target']]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.linear_model import LinearRegression

reg = LinearRegression()
reg.fit(X_train, y_train)
y_pred = reg.predict(X_test)
import matplotlib.pyplot as plt
# %matplotlib inline

#실제 주택값과 예측한 주택값 간의 상관관계

plt.scatter(y_test, y_pred, alpha=0.4)
plt.xlabel("Actual Price")
plt.ylabel("Predicted Price")
plt.title("LINEAR REGRESSION")
plt.show()

reg.coef_

plt.scatter(boston[['CRIM']], boston[['Target']], alpha=0.4)
plt.show()

plt.scatter(boston[['RM']], boston[['Target']], alpha=0.4)
plt.show()

plt.scatter(boston[['LSTAT']], boston[['Target']], alpha=0.4)
plt.show()

plt.scatter(boston[['AGE']], boston[['Target']], alpha=0.4)
plt.show()

boston.info()

plt.scatter(boston[['AGE']], boston[['TAX']], alpha=0.4)
plt.show()

plt.scatter(boston[['CRIM']], boston[['DIS']], alpha=0.4)
plt.show()

plt.scatter(boston[['RM']], boston[['DIS']], alpha=0.4)
plt.show()

# Commented out IPython magic to ensure Python compatibility.
from sklearn.model_selection import train_test_split

X = boston[['CRIM','RM', 'TAX', 'AGE']]
y = boston[['Target']]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.linear_model import LinearRegression

reg = LinearRegression()
reg.fit(X_train, y_train)
y_pred = reg.predict(X_test)
import matplotlib.pyplot as plt
# %matplotlib inline

reg.score(X_train, y_train) #train set R^2 점수
reg.score(X_test, y_test) #test set R^2 점수

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
df = pd.read_csv("penguins.csv")

df = df.dropna()

enc = LabelEncoder()
s = enc.fit_transform(df['sex'])
s
df['sex']= s

from sklearn.model_selection import train_test_split

X = df[['flipper_length_mm','bill_length_mm', 'body_mass_g', 'sex']]
y = df[['bill_depth_mm']]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Commented out IPython magic to ensure Python compatibility.
from sklearn.linear_model import LinearRegression

reg = LinearRegression()
reg.fit(X_train, y_train)
y_pred = reg.predict(X_test)
import matplotlib.pyplot as plt
# %matplotlib inline

print("TRAIN : ",reg.score(X_train, y_train)) #train set R^2 점수
print("TEST : " , reg.score(X_test, y_test)) #test set R^2 점수

plt.scatter(y_test, y_pred, alpha=0.4)
plt.xlabel("Actual Price")
plt.ylabel("Predicted Price")
plt.title("LINEAR REGRESSION")
plt.show()

reg.coef_

import pandas as pd

db = pd.read_csv('./diabetes.csv')

db.info()

db.head()

db.isna().sum(axis=0)

cols = ['Glucose', 'BloodPressure', 'SkinThickness','Insulin', 'BMI']
(db[cols] == 0).sum(axis=0)

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(8,6))
sns.countplot(x='Outcome', data = db)
plt.show()

db['Outcome'].value_counts()

X_data = db.drop(['Outcome'], axis=1)
X_data.head()

y_data = db['Outcome'] # 당뇨병 데이터에서 outcome은 y 데이터로 분류 (정답데이)
y_data.head()

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_data,y_data,
 test_size=0.2, random_state=42)
print(X_train.shape, X_test.shape)
print(y_train.shape, y_test.shape)

import numpy as np
def impute_zero(data, col):
  df = data.loc[data[col] != 0, col]
  avg = np.sum(df) / len(df)
  k = len(data.loc[ data[col] == 0, col])
  data.loc[ data[col] == 0, col ] = avg
  print('%s : fixed %d, mean: %.3f' % (col, k, avg))
for col in cols:
  impute_zero(X_train, col)

(X_train[cols] == 0).sum(axis=0)

for col in cols:
  impute_zero(X_test, col)

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X_train_s = scaler.fit_transform(X_train)
X_test_s = scaler.transform(X_test)
print(X_train_s[:5])

y_train = y_train.values
y_test = y_test.values
print(type(y_train), type(y_test))

from tensorflow import keras
from tensorflow.keras import layers
def build_model():
  model = keras.Sequential()
  model.add(layers.Dense(12, input_dim=8, activation='relu'))
  model.add(layers.Dense(8, activation='relu'))
  model.add(layers.Dense(1, activation='sigmoid'))
  return model

model = build_model()
model.summary()

model.compile(loss='binary_crossentropy',
 optimizer='adam',
 metrics=['acc'])

EPOCHS = 500
BATCH_SIZE = 16
history = model.fit(X_train_s, y_train,
 epochs=EPOCHS,
 batch_size=BATCH_SIZE,
 validation_split = 0.2,
 verbose=1)

import matplotlib.pyplot as plt
def plot_history(history):
 hist = pd.DataFrame(history.history)
 hist['epoch'] = history.epoch
 plt.figure(figsize=(16,8))
 plt.subplot(1,2,1)
 plt.xlabel('Epoch')
 plt.ylabel('Loss')
 plt.plot(hist['epoch'], hist['loss'], label='Train Loss')
 plt.plot(hist['epoch'], hist['val_loss'], label = 'Val Loss')
 plt.legend()
 plt.subplot(1,2,2)
 plt.xlabel('Epoch')
 plt.ylabel('Accuracy')
 plt.plot(hist['epoch'], hist['acc'], label='Train Accuracy')
 plt.plot(hist['epoch'], hist['val_acc'], label = 'Val Accuracy')
 plt.legend()
 plt.show()

plot_history(history)



model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['acc'])

EPOCHS = 500
BATCH_SIZE = 16
history = model.fit(X_train_s, y_train,
 epochs=EPOCHS,
 batch_size=BATCH_SIZE,
 validation_split = 0.2,
 verbose=1)

import matplotlib.pyplot as plt
def plot_history(history):
 hist = pd.DataFrame(history.history)
 hist['epoch'] = history.epoch
 plt.figure(figsize=(16,8))
 plt.subplot(1,2,1)
 plt.xlabel('Epoch')
 plt.ylabel('Loss')
 plt.plot(hist['epoch'], hist['loss'], label='Train Loss')
 plt.plot(hist['epoch'], hist['val_loss'], label = 'Val Loss')
 plt.legend()
 plt.subplot(1,2,2)
 plt.xlabel('Epoch')
 plt.ylabel('Accuracy')
 plt.plot(hist['epoch'], hist['acc'], label='Train Accuracy')
 plt.plot(hist['epoch'], hist['val_acc'], label = 'Val Accuracy')
 plt.legend()
 plt.show()

plot_history(history)

from tensorflow.keras.datasets import fashion_mnist
import tensorflow as tf

mnist = tf.keras.datasets.mnist
(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()

print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape)

class_names =['T-shirp/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']

samples = np.random.randint(len(X_train), size = 9)

plt.figure(figsize = (8,6))
for i , idx in enumerate(samples):
  plt.subplot(3,3, i+1)
  plt.xticks([])
  plt.yticks([])
  plt.imshow(X_train[idx], cmap ='gray')
  plt.title(class_names[y_train[idx]])
plt.show()

from sklearn.model_selection import train_test_split

X_train, X_val, y_train, y_val = train_test_split(
    X_train, y_train, test_size = 0.3, random_state = 42)

print(X_train.shape, y_train.shape)
print(X_val.shape, y_val.shape)

import numpy as np

X_train = X_train.astype('float32')/255
X_val = X_val.astype('float32')/255
X_test = X_test.astype('float32')/255

print(np.max(X_train), np.min(X_train))

X_train = (X_train.reshape((-1,28*28)))
X_val = (X_val.reshape((-1,28*28)))
X_test = (X_test.reshape((-1,28*28)))

print(X_train.shape, y_train.shape)
print(X_val.shape, y_val.shape)
print(X_test.shape, y_test.shape)

from tensorflow.keras.utils import to_categorical
y_train_oh = to_categorical(y_train)
y_val_oh = to_categorical(y_val)
y_test_oh = to_categorical(y_test)

y_train_oh[:5]

from tensorflow import keras
from tensorflow.keras import layers

def build_model():
  model = keras.Sequential()
  model.add(layers.Flatten(input_shape=(784,)))
  model.add(layers.Dense(64, activation ='relu',input_shape = (784,)))
  model.add(layers.Dense(32, input_shape = (784,)))
  model.add(layers.Dense(10, activation ='softmax'))
  return model

model = build_model()
model.summary()

adam = tf.keras.optimizers.Adam(learning_rate = 0.001)

model.compile(optimizer = 'adam',
              loss = 'categorical_crossentropy',
              metrics = ['acc'])

EPOCHS = 500
BATCH_SIZE =64

history = model.fit(X_train, y_train_oh,
                    epochs = EPOCHS,
                    batch_size = BATCH_SIZE,
                    validation_split = .2)

import matplotlib.pyplot as plt
import pandas as pd

def plot_history(history):
  hist = pd.DataFrame(history.history)
  hist['epoch'] = history.epoch

  plt.figure(figsize = (16,8))

  plt.subplot(1,2,1)
  plt.xlabel('Epoch')
  plt.ylabel('Loss')
  plt.plot(hist['epoch'], hist['loss'], label='Train Loss')
  plt.plot(hist['epoch'], hist['val_loss'], label='Val Loss')
  plt.legend()

  plt.subplot(1,2,2)
  plt.xlabel('Epoch')
  plt.ylabel('Accuracy')
  plt.plot(hist['epoch'], hist['acc'], label='Train Accuracy')
  plt.plot(hist['epoch'], hist['val_acc'], label='Val Accuracy')
  plt.legend()

plt.show()

plot_history(history)

y_pred = model.predict(X_test)
y_pred[:1]

y_pred_argmax = np.argmax(y_pred, axis = 1)
y_pred_argmax[:10]

n_rows = 3
n_cols = 8

n_rows = 3
n_cols = 8

plt.figure(figsize = (n_cols *2, n_rows *2))
plt.figure(figsize = (n_cols *2, n_rows *2))
for row in range(n_rows):
  for col in range(n_cols):
    index = n_cols * row + col
    plt.subplot(n_rows, n_cols, index + 1)
    plt.imshow(X_test[index].reshape(28,28), cmap = 'gray')
    #plt.imshow(X_test[index], cmap = 'gray')
    plt.axis('off')
    plt.title(f'{class_names[y_pred_argmax[index]]}({class_names[y_test[index]]})')

  plt.show()

mnist = tf.keras.datasets.mnist

from tensorflow.keras.datasets import fashion_mnist
import tensorflow as tf

mnist = tf.keras.datasets.mnist

(X_train, y_train), (X_test, y_test) = mnist.load_data()

print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape)

class_names =['0','1','2','3','4','5','6','7','8','9']

samples = np.random.randint(len(X_train), size = 9)

plt.figure(figsize = (8,6))
for i , idx in enumerate(samples):
  plt.subplot(3,3, i+1)
  plt.xticks([])
  plt.yticks([])
  plt.imshow(X_train[idx], cmap ='gray')
  plt.title(class_names[y_train[idx]])
plt.show()

from sklearn.model_selection import train_test_split

X_train, X_val, y_train, y_val = train_test_split(
    X_train, y_train, test_size = 0.3, random_state = 42)

print(X_train.shape, y_train.shape)
print(X_val.shape, y_val.shape)

import numpy as np

X_train = X_train.astype('float32')/255
X_val = X_val.astype('float32')/255
X_test = X_test.astype('float32')/255

print(np.max(X_train), np.min(X_train))

X_train = (X_train.reshape((-1,28*28)))
X_val = (X_val.reshape((-1,28*28)))
X_test = (X_test.reshape((-1,28*28)))

print(X_train.shape, y_train.shape)
print(X_val.shape, y_val.shape)
print(X_test.shape, y_test.shape)

from tensorflow import keras
from tensorflow.keras import layers

def build_model():
  model = keras.Sequential()
  model.add(layers.Flatten(input_shape=(784,)))
  model.add(layers.Dense(64, activation ='relu',input_shape = (784,)))
  model.add(layers.Dense(32, input_shape = (784,)))
  model.add(layers.Dense(10, activation ='softmax'))
  return model

model = build_model()
model.summary()

adam = tf.keras.optimizers.Adam(learning_rate = 0.001)

model.compile(optimizer = 'adam',
              loss = 'categorical_crossentropy',
              metrics = ['acc'])

EPOCHS = 100
BATCH_SIZE =64

history = model.fit(X_train, y_train_oh,
                    epochs = EPOCHS,
                    batch_size = BATCH_SIZE,
                    validation_split = .2)

import matplotlib.pyplot as plt
import pandas as pd

def plot_history(history):
  hist = pd.DataFrame(history.history)
  hist['epoch'] = history.epoch

  plt.figure(figsize = (16,8))

  plt.subplot(1,2,1)
  plt.xlabel('Epoch')
  plt.ylabel('Loss')
  plt.plot(hist['epoch'], hist['loss'], label='Train Loss')
  plt.plot(hist['epoch'], hist['val_loss'], label='Val Loss')
  plt.legend()

  plt.subplot(1,2,2)
  plt.xlabel('Epoch')
  plt.ylabel('Accuracy')
  plt.plot(hist['epoch'], hist['acc'], label='Train Accuracy')
  plt.plot(hist['epoch'], hist['val_acc'], label='Val Accuracy')
  plt.legend()

plt.show()

plot_history(history)

y_pred = model.predict(X_test)
y_pred[:1]

y_pred_argmax = np.argmax(y_pred, axis = 1)
y_pred_argmax[:10]

n_rows = 3
n_cols = 8

plt.figure(figsize = (n_cols *2, n_rows *2))
plt.figure(figsize = (n_cols *2, n_rows *2))
for row in range(n_rows):
  for col in range(n_cols):
    index = n_cols * row + col
    plt.subplot(n_rows, n_cols, index + 1)
    plt.imshow(X_test[index].reshape(28,28), cmap = 'gray')
    #plt.imshow(X_test[index], cmap = 'gray')
    plt.axis('off')
    plt.title(f'{class_names[y_pred_argmax[index]]}({class_names[y_test[index]]})')

  plt.show()

"""## week2 day2 2023 07 04"""

import numpy as np
from tensorflow.keras.datasets import fashion_mnist

(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()
print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape)

import matplotlib.pyplot as plt


class_names = ['T-shirt/top', 'Trouser', 'Pullover',
               'Dress', 'Coat', 'Sandal',
               'Shirt', 'Sneaker', 'Bag', 'Ankle boot']
samples = np.random.randint(len(X_train), size=9)
plt.figure(figsize = (8, 6))
for i, idx in enumerate(samples):
   plt.subplot(3, 3, i+1)
   plt.xticks([])
   plt.yticks([])
   plt.imshow(X_train[idx], cmap = 'gray')
   plt.title(class_names[y_train[idx]])
plt.show()

from sklearn.model_selection import train_test_split
X_train, X_val, y_train, y_val = train_test_split(
            X_train, y_train, test_size = 0.3, random_state = 42)
print(X_train.shape, y_train.shape)
print(X_val.shape, y_val.shape)

import numpy as np
X_train = X_train.astype('float32') / 255.
X_val = X_val.astype('float32') / 255.
X_test = X_test.astype('float32') / 255.
print(np.max(X_train), np.min(X_train))

print('X_train : ', X_train.shape)
print('X_val : ', X_val.shape)
print('X_test : ', X_test.shape)

import tensorflow as tf

X_train = X_train[..., tf.newaxis]
X_val = X_val[..., tf.newaxis]
X_test = X_test[..., tf.newaxis]

print('X_train : ', X_train.shape)
print('X_val : ', X_val.shape)
print('X_test : ', X_test.shape)

from tensorflow.keras.utils import to_categorical
y_train_oh = to_categorical(y_train)
y_val_oh = to_categorical(y_val)
y_test_oh = to_categorical(y_test)
y_train_oh[:5]

from tensorflow import keras
from tensorflow.keras import layers

def build_model():
  model = keras.Sequential()
  model.add(layers.Conv2D(filters = 16, kernel_size = 3,
                          strides = (1,1), padding = 'same',
                          activation ='relu',
                          input_shape = (28, 28,1)))
  model.add(layers.MaxPool2D(pool_size = (2,2), strides = 2))

  model.add(layers.Conv2D(filters = 32, kernel_size = 3,
                          strides = (1,1), padding = 'same',
                          activation ='relu',
                          input_shape = (28, 28,1)))
  model.add(layers.MaxPool2D(pool_size = (2,2), strides = 2))

  model.add(layers.Conv2D(filters = 64, kernel_size = 3,
                          strides = (1,1), padding = 'same',
                          activation ='relu',
                          input_shape = (28, 28,1)))
  model.add(layers.MaxPool2D(pool_size = (2,2), strides = 2))

  model.add(layers.Flatten())
  model.add(layers.Dense(64, activation='relu'))
  model.add(layers.Dense(10, activation='softmax'))
  return model

model = build_model()
model.summary()

adam = tf.keras.optimizers.Adam(learning_rate=0.001)
model.compile(optimizer=adam,
        loss = 'categorical_crossentropy',
        metrics=['acc'])

EPOCHS = 3
BATCH_SIZE = 64
history = model.fit(X_train, y_train_oh,
           epochs = EPOCHS,
           batch_size = BATCH_SIZE,
           validation_data = (X_val, y_val_oh),
           verbose = 1)

import matplotlib.pyplot as plt
import pandas as pd
def plot_history(history):
  hist = pd.DataFrame(history.history)
  hist['epoch'] = history.epoch
  plt.figure(figsize=(16,8))
  plt.subplot(1,2,1)
  plt.xlabel('Epoch')
  plt.ylabel('Loss')
  plt.plot(hist['epoch'], hist['loss'], label='Train Loss')
  plt.plot(hist['epoch'], hist['val_loss'],label = 'Val Loss')
  plt.legend()

  plt.subplot(1,2,2)
  plt.xlabel('Epoch')
  plt.ylabel('Accuracy')
  plt.plot(hist['epoch'], hist['acc'], label='Train Accuracy')
  plt.plot(hist['epoch'], hist['val_acc'], label = 'Val Accuracy')
  plt.legend()
plt.show()

plot_history(history)

from tensorflow import keras
from tensorflow.keras import layers
def build_dropout_model():
   model = keras.Sequential()
   model.add(layers.Conv2D(filters=16,
                           kernel_size= 3,
                           strides=(1, 1),
                           padding='same',
                           activation='relu',
                           input_shape=(28, 28, 1)))
   model.add(layers.MaxPool2D(pool_size=(2, 2), strides=2))
   model.add(layers.Dropout(0.2))

   model.add(layers.Conv2D(filters=32,
                           kernel_size= 3,
                           strides=(1, 1),
                           padding='same',
                           activation='relu'))
   model.add(layers.MaxPool2D(pool_size=(2, 2), strides=2))
   model.add(layers.Dropout(0.2))

   model.add(layers.Conv2D(filters=64,
                           kernel_size= 3,
                           strides=(1, 1),
                           padding='same',
                           activation='relu'))
   model.add(layers.MaxPool2D(pool_size=(2, 2), strides=2))
   model.add(layers.Dropout(0.2))

   model.add(layers.Flatten())
   model.add(layers.Dense(64, activation = 'relu'))
   model.add(layers.Dense(10, activation = 'softmax'))
   return model

model = build_dropout_model()
model.summary()

model.compile(optimizer='adam',
              loss = 'categorical_crossentropy',
              metrics=['acc'])

EPOCHS = 30
BATCH_SIZE = 64
history = model.fit(X_train, y_train_oh,
                    epochs = EPOCHS,
                    batch_size = BATCH_SIZE,
                    validation_data = (X_val, y_val_oh),
                    verbose = 1)

import matplotlib.pyplot as plt
import pandas as pd
def plot_history(history):
  hist = pd.DataFrame(history.history)
  hist['epoch'] = history.epoch
  plt.figure(figsize=(16,8))
  plt.subplot(1,2,1)
  plt.xlabel('Epoch')
  plt.ylabel('Loss')
  plt.plot(hist['epoch'], hist['loss'], label='Train Loss')
  plt.plot(hist['epoch'], hist['val_loss'],label = 'Val Loss')
  plt.legend()

  plt.subplot(1,2,2)
  plt.xlabel('Epoch')
  plt.ylabel('Accuracy')
  plt.plot(hist['epoch'], hist['acc'], label='Train Accuracy')
  plt.plot(hist['epoch'], hist['val_acc'], label = 'Val Accuracy')
  plt.legend()
plt.show()

plot_history(history)

y_pred = model.predict(X_test)
y_pred_argmax = np.argmax(y_pred, axis=1)
y_pred_argmax[:10]

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
def print_score(y_test, y_pred):
   print('accuracy: %.3f' % (accuracy_score(y_test, y_pred)))
   print('precision: %.3f' % (precision_score(y_test, y_pred, average='macro')))
   print('recall_score: %.3f' % (recall_score(y_test, y_pred, average='macro')))
   print('f1_score: %.3f' % (f1_score(y_test, y_pred, average='macro')))

print_score(y_test, y_pred_argmax)

# mnist 사용 및 결과값 비교

import numpy as np
from tensorflow.keras.datasets import mnist

(X_train, y_train), (X_test, y_test) = mnist.load_data()
print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape)

import matplotlib.pyplot as plt


class_names = ['0', '1', '2',
               '3', '4', '5',
               '6', '7', '8', '9']
samples = np.random.randint(len(X_train), size=9)
plt.figure(figsize = (8, 6))
for i, idx in enumerate(samples):
   plt.subplot(3, 3, i+1)
   plt.xticks([])
   plt.yticks([])
   plt.imshow(X_train[idx], cmap = 'gray')
   plt.title(class_names[y_train[idx]])
plt.show()

from sklearn.model_selection import train_test_split
X_train, X_val, y_train, y_val = train_test_split(
            X_train, y_train, test_size = 0.3, random_state = 42)
print(X_train.shape, y_train.shape)
print(X_val.shape, y_val.shape)

import numpy as np
X_train = X_train.astype('float32') / 255.
X_val = X_val.astype('float32') / 255.
X_test = X_test.astype('float32') / 255.
print(np.max(X_train), np.min(X_train))

print('X_train : ', X_train.shape)
print('X_val : ', X_val.shape)
print('X_test : ', X_test.shape)

import tensorflow as tf

X_train = X_train[..., tf.newaxis]
X_val = X_val[..., tf.newaxis]
X_test = X_test[..., tf.newaxis]

print('X_train : ', X_train.shape)
print('X_val : ', X_val.shape)
print('X_test : ', X_test.shape)

from tensorflow.keras.utils import to_categorical
y_train_oh = to_categorical(y_train)
y_val_oh = to_categorical(y_val)
y_test_oh = to_categorical(y_test)
y_train_oh[:5]

from tensorflow import keras
from tensorflow.keras import layers
def build_dropout_model():
   model = keras.Sequential()
   model.add(layers.Conv2D(filters=16,
                           kernel_size= 3,
                           strides=(1, 1),
                           padding='same',
                           activation='relu',
                           input_shape=(28, 28, 1)))
   model.add(layers.MaxPool2D(pool_size=(2, 2), strides=2))
   model.add(layers.Dropout(0.2))

   model.add(layers.Conv2D(filters=32,
                           kernel_size= 3,
                           strides=(1, 1),
                           padding='same',
                           activation='relu'))
   model.add(layers.MaxPool2D(pool_size=(2, 2), strides=2))
   model.add(layers.Dropout(0.2))

   model.add(layers.Conv2D(filters=64,
                           kernel_size= 3,
                           strides=(1, 1),
                           padding='same',
                           activation='relu'))
   model.add(layers.MaxPool2D(pool_size=(2, 2), strides=2))
   model.add(layers.Dropout(0.2))

   model.add(layers.Flatten())
   model.add(layers.Dense(64, activation = 'relu'))
   model.add(layers.Dense(10, activation = 'softmax'))
   return model

model = build_model()
model.summary()

adam = tf.keras.optimizers.Adam(learning_rate=0.001)
model.compile(optimizer=adam,
        loss = 'categorical_crossentropy',
        metrics=['acc'])

EPOCHS = 30
BATCH_SIZE = 64
history = model.fit(X_train, y_train_oh,
                    epochs = EPOCHS,
                    batch_size = BATCH_SIZE,
                    validation_data = (X_val, y_val_oh),
                    verbose = 1)

import matplotlib.pyplot as plt
import pandas as pd
def plot_history(history):
  hist = pd.DataFrame(history.history)
  hist['epoch'] = history.epoch
  plt.figure(figsize=(16,8))
  plt.subplot(1,2,1)
  plt.xlabel('Epoch')
  plt.ylabel('Loss')
  plt.plot(hist['epoch'], hist['loss'], label='Train Loss')
  plt.plot(hist['epoch'], hist['val_loss'],label = 'Val Loss')
  plt.legend()

  plt.subplot(1,2,2)
  plt.xlabel('Epoch')
  plt.ylabel('Accuracy')
  plt.plot(hist['epoch'], hist['acc'], label='Train Accuracy')
  plt.plot(hist['epoch'], hist['val_acc'], label = 'Val Accuracy')
  plt.legend()
plt.show()

plot_history(history)

y_pred = model.predict(X_test)
y_pred_argmax = np.argmax(y_pred, axis=1)
y_pred_argmax[:10]

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
def print_score(y_test, y_pred):
   print('accuracy: %.3f' % (accuracy_score(y_test, y_pred)))
   print('precision: %.3f' % (precision_score(y_test, y_pred, average='macro')))
   print('recall_score: %.3f' % (recall_score(y_test, y_pred, average='macro')))
   print('f1_score: %.3f' % (f1_score(y_test, y_pred, average='macro')))

print_score(y_test, y_pred_argmax)

import numpy as np
import matplotlib.pyplot as plt
from keras.datasets import cifar10

(X_train, y_train), (X_test, y_test) = cifar10.load_data()
print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape)

import numpy as np
import matplotlib.pyplot as plt
from keras.datasets import cifar10

(X_train, y_train), (X_test, y_test) = cifar10.load_data()
print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape)

X_train = X_train.astype('float32') / 255.0
X_test = X_test.astype('float32') / 255.0

from sklearn.model_selection import train_test_split
X_train, X_val, y_train, y_val = train_test_split(
           X_train, y_train, test_size = 0.3,  random_state = 42)

print(X_train.shape, y_train.shape)
print(X_val.shape, y_val.shape)
print(X_test.shape, y_test.shape)

y_train = y_train.reshape(-1)
y_val = y_val.reshape(-1)
y_test = y_test.reshape(-1)
print(y_train.shape)
print(y_val.shape)
print(y_test.shape)

import tensorflow as tf
y_train_oh = tf.one_hot(y_train, depth=10)
y_val_oh = tf.one_hot(y_val, depth=10)
y_test_oh = tf.one_hot(y_test, depth=10)
print(y_train_oh.shape)
y_train_oh[:5]

y_train_oh = y_train_oh.numpy()
y_val_oh = y_val_oh.numpy()
y_test_oh = y_test_oh.numpy()
print(y_train_oh.shape)
print(y_val_oh.shape)
print(y_test_oh.shape)

from keras import layers

def build_model():
   model = keras.Sequential()

   model.add(layers.Conv2D(32, 3, padding = 'same',
         activation='relu', input_shape = (32, 32, 3)))
   model.add(layers.MaxPooling2D(2))
   model.add(layers.Dropout(0.3))

   model.add(layers.Conv2D(64, 3, padding = 'same',
         activation='relu'))
   model.add(layers.MaxPooling2D(2))
   model.add(layers.Dropout(0.3))

   model.add(layers.Conv2D(256, 3, padding = 'same',
         activation='relu'))
   model.add(layers.MaxPooling2D(2))
   model.add(layers.Dropout(0.3))

   model.add(layers.Conv2D(256, 3, padding = 'same',
         activation='relu'))
   model.add(layers.MaxPooling2D(2))
   model.add(layers.Dropout(0.3))

   model.add(layers.Flatten())
   model.add(layers.Dense(256, activation='relu'))
   model.add(layers.Dense(10, activation='softmax'))
   return model

model = build_model()
model.summary()

adam = tf.keras.optimizers.Adam(learning_rate=0.0001)
model.compile(optimizer=adam,
         loss = 'categorical_crossentropy',
         metrics=['acc'])
EPOCHS = 100
BATCH_SIZE = 256
history = model.fit(X_train, y_train_oh,
           epochs = EPOCHS,
           batch_size = BATCH_SIZE,
           validation_data = (X_val, y_val_oh))

model = build_model()
model.summary()

plot_history(history)

y_pred = model.predict(X_test)
y_pred_argmax = np.argmax(y_pred, axis=1)

from sklearn.metrics import confusion_matrix
import seaborn as sns
def plot_matrix(y_test, y_pred):
   plt.figure(figsize = (10, 8))
   cm = confusion_matrix(y_test, y_pred)
   sns.heatmap(cm, annot = True, fmt = 'd',cmap = 'Blues')
   plt.xlabel('predicted label', fontsize = 15)
   plt.ylabel('true label', fontsize = 15)
   plt.show()

plot_matrix(y_test, y_pred_argmax)

model = build_model()
adam = tf.keras.optimizers.Adam(learning_rate=0.0001)
model.compile(optimizer=adam,
         loss = 'categorical_crossentropy',
         metrics=['acc'])

from keras import callbacks

checkpoint_path = 'temp/cifar_10.ckpt'
checkpoint = callbacks.ModelCheckpoint(checkpoint_path,
                                       save_weights_only=True,
                                       save_best_only=True,
                                       monitor='val_loss')

EPOCHS = 100
BATCH_SIZE = 256
history = model.fit(X_train, y_train_oh,
                    epochs = EPOCHS,
                    batch_size = BATCH_SIZE,
                    validation_data = (X_val, y_val_oh),
                    callbacks=[checkpoint])

model.load_weights(checkpoint_path)

y_pred = model.predict(X_test)
y_pred_argmax = np.argmax(y_pred, axis=1)

print_score(y_test, y_pred_argmax)

plot_history(history)

"""# **※ 실습 ( 개와 고양이 분류 ^^ )**"""

from google.colab import drive
drive.mount('/content/drive')

!wget --no-check-certificate \
https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \
-O /tmp/cats_and_dogs_filtered.zip

import os
import zipfile

local_zip = '/tmp/cats_and_dogs_filtered.zip'

zip_ref = zipfile.ZipFile(local_zip, 'r')

zip_ref.extractall('/tmp')
zip_ref.close()

# 기본 경로
base_dir = '/tmp/cats_and_dogs_filtered'

train_dir = os.path.join(base_dir, 'train')
validation_dir = os.path.join(base_dir, 'validation')

# 훈련에 사용되는 고양이/개 이미지 경로
train_cats_dir = os.path.join(train_dir, 'cats')
train_dogs_dir = os.path.join(train_dir, 'dogs')
print(train_cats_dir)
print(train_dogs_dir)

# 테스트에 사용되는 고양이/개 이미지 경로
validation_cats_dir = os.path.join(validation_dir, 'cats')
validation_dogs_dir = os.path.join(validation_dir, 'dogs')
print(validation_cats_dir)
print(validation_dogs_dir)

train_cat_fnames = os.listdir( train_cats_dir )
train_dog_fnames = os.listdir( train_dogs_dir )

print(train_cat_fnames[:5])
print(train_dog_fnames[:5])

print('Total training cat images :', len(os.listdir(train_cats_dir)))
print('Total training dog images :', len(os.listdir(train_dogs_dir)))

print('Total validation cat images :', len(os.listdir(validation_cats_dir)))
print('Total validation dog images :', len(os.listdir(validation_dogs_dir)))

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

import matplotlib.image as mpimg
import matplotlib.pyplot as plt

nrows, ncols = 4, 4
pic_index = 0

fig = plt.gcf()
fig.set_size_inches(ncols*3, nrows*3)

pic_index+=8

next_cat_pix = [os.path.join(train_cats_dir, fname)
                for fname in train_cat_fnames[ pic_index-8:pic_index]]

next_dog_pix = [os.path.join(train_dogs_dir, fname)
                for fname in train_dog_fnames[ pic_index-8:pic_index]]

for i, img_path in enumerate(next_cat_pix+next_dog_pix):
  sp = plt.subplot(nrows, ncols, i + 1)
  sp.axis('Off')

  img = mpimg.imread(img_path)
  plt.imshow(img)

plt.show()

import tensorflow as tf


model = tf.keras.models.Sequential([
  tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150, 150, 3)),
  tf.keras.layers.MaxPooling2D(2,2),
  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),
  tf.keras.layers.MaxPooling2D(2,2),
  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),
  tf.keras.layers.MaxPooling2D(2,2),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(512, activation='relu'),
  tf.keras.layers.Dense(1, activation='sigmoid')
])

model.summary()

from tensorflow.keras.optimizers import RMSprop

model.compile(optimizer=RMSprop(lr=0.001),
            loss='binary_crossentropy',
            metrics = ['accuracy'])

from tensorflow.keras.preprocessing.image import ImageDataGenerator


train_datagen = ImageDataGenerator( rescale = 1.0/255. )
test_datagen  = ImageDataGenerator( rescale = 1.0/255. )

train_generator = train_datagen.flow_from_directory(train_dir,
                                                  batch_size=20,
                                                  class_mode='binary',
                                                  target_size=(150, 150))
validation_generator =  test_datagen.flow_from_directory(validation_dir,
                                                       batch_size=20,
                                                       class_mode  = 'binary',
                                                       target_size = (150, 150))

from keras import callbacks

history = model.fit(train_generator,
                    validation_data=validation_generator,
                    steps_per_epoch=100,
                    epochs=100,
                    validation_steps=50,
                    verbose=2)

import matplotlib.pyplot as plt

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(len(acc))

plt.plot(epochs, acc, 'bo', label='Training accuracy')
plt.plot(epochs, val_acc, 'b', label='Validation accuracy')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'go', label='Training Loss')
plt.plot(epochs, val_loss, 'g', label='Validation Loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()

"""# week2 day2 2023 07 05"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
from PIL import Image , ImageDraw
from sklearn.preprocessing import *
import time
import ast
import os
import keras
import tensorflow as tf
from keras import models, layers
from keras import Input
from keras.models import Model, load_model
from keras.preprocessing.image import ImageDataGenerator
from keras import optimizers, initializers, regularizers, metrics
from keras.callbacks import ModelCheckpoint, EarlyStopping
from keras.layers import BatchNormalization, Conv2D, Activation , AveragePooling2D , Input ,Dropout
from keras.layers import Dense, GlobalAveragePooling2D, MaxPooling2D, ZeroPadding2D, Add, Flatten
from keras.models import Sequential
from keras.metrics import top_k_categorical_accuracy
from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping
from keras.applications import ResNet50, vgg19,mobilenet_v2,InceptionV3 , InceptionResNetV2,DenseNet169
from tqdm import tqdm
import cv2

!pip install tensorflow_datasets

import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
import tensorflow_datasets as tfds

from tensorflow.keras import layers
from tensorflow.keras.datasets import mnist

(train_ds, val_ds, test_ds), metadata = tfds.load(
    'tf_flowers',
    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],
    with_info=True,
    as_supervised=True,
)

get_label_name = metadata.features['label'].int2str

image, label = next(iter(train_ds))
_ = plt.imshow(image)
_ = plt.title(get_label_name(label))

# RandomFlip을 통해 수평 또는 수직으로 주어진 이미지를 flip합니다.
# RandomRotation을 통해 이미지에 회전효과를 줍니다.
data_augmentation = tf.keras.Sequential([
  layers.experimental.preprocessing.RandomFlip("horizontal_and_vertical"),
  layers.experimental.preprocessing.RandomRotation(0.2),
])

# 이전 셀에서 증강시킨 이미지 데이터를 출력하여 어떤 변화가 있는지 확인해봅니다.
image = tf.expand_dims(image, 0)

plt.figure(figsize=(10, 10))
for i in range(9):
  augmented_image = data_augmentation(image)
  ax = plt.subplot(3, 3, i + 1)
  plt.imshow(augmented_image[0])
  plt.axis("off")

import cv2
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.image import ImageDataGenerator

#cv2.imread는 이미지를 RGB가 아닌 BGR로 받아오기 때문에 바꿔 주어야함.
image = cv2.cvtColor(cv2.imread('dog.jpg'), cv2.COLOR_BGR2RGB)
plt.imshow(image)

#augmentation이 적용된 image들을 시각화 해주는 함수
def show_aug_image(image, generator, n_images=4):

    # ImageDataGenerator는 여러개의 image를 입력으로 받기 때문에 4차원으로 입력 해야함.
    image_batch = np.expand_dims(image, axis=0)

    # featurewise_center or featurewise_std_normalization or zca_whitening 가 True일때만 fit 해주어야함
    generator.fit(image_batch)
    # flow로 image batch를 generator에 넣어주어야함.
    data_gen_iter = generator.flow(image_batch)

    fig, axs = plt.subplots(nrows=1, ncols=n_images, figsize=(24, 8))

    for i in range(n_images):
    	#generator에 batch size 만큼 augmentation 적용(매번 적용이 다름)
        aug_image_batch = next(data_gen_iter)
        aug_image = np.squeeze(aug_image_batch)
        aug_image = aug_image.astype('int')
        axs[i].imshow(aug_image)
data_generator = ImageDataGenerator(horizontal_flip=True)
show_aug_image(image, data_generator, n_images=4)

data_generator = ImageDataGenerator(vertical_flip=True)
show_aug_image(image, data_generator, n_images=4)

data_generator = ImageDataGenerator(horizontal_flip=True, vertical_flip=True)
show_aug_image(image, data_generator, n_images=4)

data_generator = ImageDataGenerator(rotation_range=45)
show_aug_image(image, data_generator, n_images=4)

data_generator = ImageDataGenerator(width_shift_range=0.4, fill_mode='nearest')
show_aug_image(image, data_generator, n_images=4)

data_generator = ImageDataGenerator(width_shift_range=0.4, fill_mode='reflect')
show_aug_image(image, data_generator, n_images=4)

data_generator = ImageDataGenerator(width_shift_range=0.4, fill_mode='wrap')
show_aug_image(image, data_generator, n_images=4)

data_generator = ImageDataGenerator(width_shift_range=0.4, fill_mode='constant', cval=0)
show_aug_image(image, data_generator, n_images=4)

data_generator = ImageDataGenerator(shear_range=45)
show_aug_image(image, data_generator, n_images=4)

data_generator = ImageDataGenerator(zoom_range=[0.5, 0.9])
show_aug_image(image, data_generator, n_images=4)

data_generator = ImageDataGenerator(zoom_range=[1.1, 1.5], fill_mode='constant', cval=0)
show_aug_image(image, data_generator, n_images=4)

ata_generator = ImageDataGenerator(brightness_range=(0.1, 0.9))
show_aug_image(image, data_generator, n_images=4)

data_generator = ImageDataGenerator(brightness_range=(1.0, 1.0))
show_aug_image(image, data_generator, n_images=4)

data_generator = ImageDataGenerator(brightness_range=(1.0, 2.0))
show_aug_image(image, data_generator, n_images=4)

data_generator = ImageDataGenerator(channel_shift_range=150)
show_aug_image(image, data_generator, n_images=4)

data_generator = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    brightness_range=(0.7, 1.3),
    horizontal_flip=True,
    vertical_flip=True,
    #rescale=1/255.0 # 학습시 적용, 시각화를 위해 임시로 주석처리
)

show_aug_image(image, data_generator, n_images=4)

"""**※ Kaggle 설치**"""

!pip install kaggle --upgrade

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!ls -1ha kaggle.json

"""# 전의 학습^^"""

import cv2
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.image import ImageDataGenerator

#cv2.imread는 이미지를 RGB가 아닌 BGR로 받아오기 때문에 바꿔 주어야함.
image = cv2.cvtColor(cv2.imread('dog.jpg'), cv2.COLOR_BGR2RGB)
plt.imshow(image)

#augmentation이 적용된 image들을 시각화 해주는 함수
def show_aug_image(image, generator, n_images=4):

    # ImageDataGenerator는 여러개의 image를 입력으로 받기 때문에 4차원으로 입력 해야함.
    image_batch = np.expand_dims(image, axis=0)

    # featurewise_center or featurewise_std_normalization or zca_whitening 가 True일때만 fit 해주어야함
    generator.fit(image_batch)
    # flow로 image batch를 generator에 넣어주어야함.
    data_gen_iter = generator.flow(image_batch)

    fig, axs = plt.subplots(nrows=1, ncols=n_images, figsize=(24, 8))

    for i in range(n_images):
    	#generator에 batch size 만큼 augmentation 적용(매번 적용이 다름)
        aug_image_batch = next(data_gen_iter)
        aug_image = np.squeeze(aug_image_batch)
        aug_image = aug_image.astype('int')
        axs[i].imshow(aug_image)

data_generator = ImageDataGenerator(horizontal_flip=True)
show_aug_image(image, data_generator, n_images=4)

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import matplotlib.pyplot as plt
import tensorflow_datasets as tfds
tfds.disable_progress_bar()

import tensorflow as tf

keras = tf.keras

!unzip -qq "/dogs-vs-cats.zip"

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/Colab Notebooks/SKT FLY AI

!unzip -qq "/content/drive/MyDrive/Colab Notebooks/SKT FLY AI/train.zip"

#training, validtation, test를 8:1:1로 분할합니다.
(raw_train, raw_validation, raw_test), metadata = tfds.load(
    'cats_vs_dogs',
    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],
    with_info = True,
    as_supervised = True,
)

get_label_name = metadata.features['label'].int2str

for image, label in raw_train.take(2):
    plt.figure()
    plt.imshow(image)
    plt.title(get_label_name(label))

print(raw_train, raw_validation, raw_test, sep ='\n')

#<PrefetchDataset shapes: ((None, None, 3), ()), types: (tf.uint8, tf.int64)>

import matplotlib.pyplot as plt
plt.figure(figsize=(10,5))

get_label_name = metadata.features['label'].int2str
# take() 새로운 데이터 셋 생성
for idx, (image, label) in enumerate(raw_train.take(10)):
    plt.subplot(2, 5, idx+1)
    plt.imshow(image)
    plt.title(f'label {label}: {get_label_name(label)}')
    plt.axis('off')

img_size = 160

def format_example(image, label):
    image = tf.cast(image, tf.float32) #고양이의 tensorflow 타입
    image = (image/127.5) - 1 # pixel scale -1 ~ 1범위로 수정
    image = tf.image.resize(image, (160, 160))
    return image, label

train = raw_train.map(format_example)
validation = raw_validation.map(format_example)
test = raw_test.map(format_example)

print(train, validation, test, sep = '\n')

plt.figure(figsize = (10, 5))

get_label_name = metadata.features['label'].int2str

for idx, (image, label) in enumerate(train.take(10)):
    plt.subplot(2, 5, idx+1)
    # image = (image + 1) / 2
    plt.imshow(image)
    plt.title(f'label {label}: {get_label_name(label)}')
    plt.axis('off')

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D,Flatten

Model = Sequential([
                    Conv2D(filters = 16,
                           kernel_size = 3,
                           padding = 'same',
                           activation ='relu',
                           input_shape = (160,160,3)),
                    MaxPooling2D(),
                    Conv2D(filters = 32,
                           kernel_size = 3,
                           padding = 'same',
                           activation = 'relu'),
                    MaxPooling2D(),
                    Conv2D(filters = 64,
                           kernel_size = 3,
                           padding = 'same',
                           activation = 'relu'),
                    MaxPooling2D(),
                    Flatten(),
                    Dense(units = 512,
                          activation = 'relu'),
                    Dense(units=2,
                          activation = 'softmax')
])
Model.summary()

"""**※ 특성 평탄화**"""

Model.compile(optimizer = tf.keras.optimizers.RMSprop(lr = 0.0001),
              loss = tf.keras.losses.sparse_categorical_crossentropy,
              metrics = ['accuracy'])

BATCH_SIZE = 32
SHUFFLE_BUFFER_SIZE = 1000
train_batches = train.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)
validation_batches = validation.batch(BATCH_SIZE)
test_batches = test.batch(BATCH_SIZE)

for image_batch, label_batch in train_batches.take(1):
    pass

image_batch.shape, label_batch.shape

validation_steps = 20
loss0, accuracy0 = Model.evaluate(validation_batches, steps=validation_steps)
print("initial loss: {:.2f}".format(loss0),"initial accuracy{:.2f}".format(accuracy0),sep="\n")

epoch = 10
history = Model.fit(train_batches,
                    epochs = epoch,
                    validation_data = validation_batches)

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss=history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(epoch)

plt.figure(figsize = (12, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label = 'Training Accuracy')
plt.plot(epochs_range, val_acc, label = 'Validation Accuracy')
plt.legend()
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label = 'Training Loss')
plt.plot(epochs_range, val_loss, label = 'Validation Loss')
plt.legend()
plt.title('Training and Validation Loss')
plt.show()

for image_batch, label_batch in test_batches.take(1):
    images = image_batch
    labels = label_batch
    predictions = Model.predict(image_batch)
    pass

predictions = np.argmax(predictions, axis=1)

plt.figure(figsize=(20,12))

for idx, (image, label, prediction) in enumerate(zip(images, labels, predictions)):
    plt.subplot(4, 8, idx+1)
    image = (image + 1) / 2 # make image positive
    plt.imshow(image)
    correct = label == prediction
    title = f'real:{label} / pred :{prediction}\n {correct}!'
    if not correct:
        plt.title(title, fontdict={'color':'red'})
    else:
        plt.title(title,fontdict={'color':'blue'})
        plt.axis('off')

from google.colab import drive
drive.mount('/content/drive')

"""**※ git 연동**"""

cd /content/drive/MyDrive/Colab Notebooks/SKT FLY AI/data

!git clone https://ohjungmin317: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@github.com/ohjungmin317/SKT_FLY_AI_ML.git

cd /content/drive/MyDrive/Colab Notebooks/SKT FLY AI/data/SKT_FLY_AI_ML

!git config --global user.email 'jmin980317@naver.com'
!git config --global user.name 'ohjungmin317'

!git add --all

!git commit -m 'data_file'
!git push

pwd

cd ..

cd /content/drive/MyDrive/Colab Notebooks/SKT FLY AI/data/

!git add circle.png

!git commit -m 'data_file_uploads'
!git push -u origin main

!git remote -v

!git remote remove origin

!git remote add origin https://github.com/ohjungmin317/SKT_FLY_AI_ML.git

!git remote -v

!git add --all

!git commit -m 'data_file_uploads'
!git push -u origin master --force

!git status

!git log

!git status

!git config --global user.email "jmin980317@naver.com"
!git config --global user.name "ohjungmin317"
!git add .
!git commit -m "data_file_uploads"
# !git push https://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@github.com/ohjungmin317/SKT_FLY_AI_ML.git

!git push -u origin master --force

"""**※ 이미지 전처리**"""

from tensorflow.keras.preprocessing import image
import numpy as np

sodal = image.load_img('sodal.jpg')
sodal_arr = np.asarray(image.img_to_array(dog))
sodal_arr = sodal_arr/255
print(sodal_arr.shape)

import matplotlib.pyplot as plt

plt.imshow(sodal_arr)
plt.show()

""" **1. 패딩**

**- 이미지의 외곽을 특정한 색상으로**
"""

from tensorflow.image import pad_to_bounding_box
target_height = 1500
target_width = 1500
source_height = sodal_arr.shape[0]
source_width = sodal_arr.shape[1]
sodal_arr_pad = pad_to_bounding_box(sodal_arr,
         int((target_height-source_height)/2),
         int((target_width-source_width)/2),
         target_height,
         target_width)
print(sodal_arr_pad.shape)

plt.imshow(sodal_arr_pad)
plt.show()

"""**#2. cropping**

**- 가운데를 중심으로 정사각형으로 자름**<br>
**- 가운데를 중심으로 50%를 자르기 때문에 가로와 세로의 비율이 유지**
"""

from tensorflow.image import central_crop

sodal_arr_crop = central_crop(sodal_arr,0.5)
print(sodal_arr_crop.shape)

plt.imshow(sodal_arr_crop)
plt.show()

"""**가로 세로 중에 짧은 쪽을 기준으로 정사각형으로 자르기**

"""

w,h = sodal.size
s = min(w,h)
y = (h-s)
x = (w-s)

print(w,h,x,y,s)

sodal_crop = sodal.crop((x,y,x+s,y+s))
print(sodal_crop.size)

plt.imshow(np.asarray(sodal_crop))
plt.show()

"""**Warping**<br>
<b>원본 이미지를 정사각형으로 확대 또는 축소. 이미지의 왜곡이 발생</b>
"""

from tensorflow.image import resize
sodal_arr_resize = resize(sodal_arr, (300,300))
print(sodal_arr_resize.shape)

plt.imshow(sodal_arr_resize)
plt.show()